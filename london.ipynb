{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a167ac7-74c9-44ce-98e2-24f97e075e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tar chvfz notebook.tar.gz *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae39d5d-e635-4ba8-9e19-e5b32a5a18a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # Remove chain setting warning\n",
    "import numpy as np\n",
    "import missingno as msno\n",
    "import plotly as py\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "from optuna.samplers import TPESampler\n",
    "from geopy.geocoders import Nominatim\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, log_loss, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm # Ok, Billy you need to deal with this somehow ... I guess getting access to a GPU machine will work\n",
    "import time\n",
    "\n",
    "r_s = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea1bd3e-5a3f-4e46-adb1-28aca30eacb5",
   "metadata": {},
   "source": [
    "## Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472a2258-23b5-455c-a71c-c94e1947451b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load in housing data\n",
    "\n",
    "house_data = pd.read_csv(\"housing_in_london/housing_in_london_yearly_variables.csv\") #billy you need to cut sub 2001 data?\n",
    "house_data.rename(columns={\"area\": \"borough_name\"}, inplace=True)\n",
    "house_data.borough_name = house_data.borough_name.str.title()\n",
    "house_data.borough_name = house_data.borough_name.replace(['Of','And'],['of','and'], regex=True)\n",
    "house_data.dtypes # Lets check datatypes. FOr some reason mean salary and recylcing percentages are objects not floats. Lets change that!\n",
    "house_data.mean_salary = house_data.mean_salary.apply(pd.to_numeric, errors='coerce') #Drop strings and make float\n",
    "house_data.recycling_pct = house_data.recycling_pct.apply(pd.to_numeric, errors='coerce') #Drop strings and make float\n",
    "\n",
    "\n",
    "house_data.borough_name.unique() #Some of these aren't London boroughs\n",
    "\n",
    "remove_boroughs = ['North East',\n",
    "       'North West', 'Yorkshire and The Humber', 'East Midlands',\n",
    "       'West Midlands', 'East', 'South East', 'South West',\n",
    "       'Inner London', 'Outer London', 'United Kingdom', 'Great Britain',\n",
    "       'England and Wales', 'Northern Ireland', 'Scotland', 'Wales', 'England', 'London']\n",
    "\n",
    "house_data = house_data[~house_data[\"borough_name\"].isin(remove_boroughs)]\n",
    "\n",
    "# Change dates to just years\n",
    "house_data['year'] = house_data['date'].str.split('-').str[0]\n",
    "house_data.drop(columns = [\"date\", \"borough_flag\"], inplace = True)\n",
    "\n",
    "\n",
    "msno.matrix(house_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8ef56d-cce4-4562-868d-f541ee1a9946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do visualisation for the two datasets seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9541d6e7-0e3f-4ece-bbf6-19d0316e3ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7347018d-05d0-4f54-9f3b-f19b851613ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# FInally, we need to add an \"election\" column\n",
    "\n",
    "def add_election(row):\n",
    "    if int(row.year) < 2006:\n",
    "        election = \"2005\"\n",
    "        return election\n",
    "    elif int(row.year) < 2011:\n",
    "        election = \"2010\"\n",
    "        return election\n",
    "    elif int(row.year) < 2016:\n",
    "        election = \"2015\"\n",
    "        return election\n",
    "    elif int(row.year) < 2018:\n",
    "        election = \"2017\"\n",
    "        return election\n",
    "    else:\n",
    "        election = \"2019\"\n",
    "        return election\n",
    "        \n",
    "house_data[\"election\"] = house_data.apply(lambda row: add_election(row), axis=1)\n",
    "\n",
    "\n",
    "msno.matrix(house_data)\n",
    "#Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3fbf4d-c570-4957-9880-40a92b1b3c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's first drop the entire \"life_satisfaction\" column here as it is missing lots of values. We can then remove individual rows that contain a \"NaN.\n",
    "house_data.drop(columns = \"life_satisfaction\", inplace = True)\n",
    "\n",
    "house_data.hist(figsize=(15,15))\n",
    "\n",
    "#All the features with missing values skew, so let's try replacing all with median\n",
    "house_data.fillna(house_data.median(),inplace=True)\n",
    "plt.show()\n",
    "\n",
    "#house_data = house_data.fillna(house_data.groupby('borough_name').transform('mean'))\n",
    "\n",
    "msno.matrix(house_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f9f2aa-0874-4766-9c17-73dde8f93d16",
   "metadata": {},
   "source": [
    "Load in election data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9da8c9-b476-43a8-a1f4-dc7662b74d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in election data\n",
    "\n",
    "election_data = pd.read_csv(\"1918-2019election_results.csv\",encoding = \"ISO-8859-1\") #Needs encoding specified or wont read file\n",
    "election_data.rename(columns={'lib_votes ':'lib_votes'}, inplace=True)\n",
    "election_data = election_data[election_data[\"election\"]>\"2001\"] #this now matches other data\n",
    "election_data = election_data[election_data[\"country/region\"]=='London'] #Only London\n",
    "election_data = election_data.astype({'lib_votes': float, 'lab_votes': float})\n",
    "election_data = election_data[[\"constituency_id\", \"constituency_name\", \"con_votes\", \"lib_votes\" ,\"lab_votes\", \"election\"]] # We want the share (percentage of votes rather than raw number)\n",
    "election_data.constituency_name = election_data.constituency_name.replace(['&','And'],'and', regex=True)\n",
    "election_data.constituency_name = election_data.constituency_name.replace('Of','of', regex=True)\n",
    "#Load in constituency list for conversion\n",
    "\n",
    "constituencies = pd.read_excel(\"PCON_DEC_2021_UK_NC.xlsx\", sheet_name=\"PCON_DEC_2021_UK_NC\", names=[\"constituency_code\", \"constituency_name\"])\n",
    "constituencies.constituency_name = constituencies.constituency_name.replace(',','', regex=True)\n",
    "dict_name_code = dict(zip(constituencies.constituency_name,constituencies.constituency_code))\n",
    "\n",
    "# Replace old constituency with closest modern analogue\n",
    "\n",
    "dict_replace_old_constituencies = {'Brent East':'Chelsea and Fulham', #Billy this line is a fudge, but come back to it and fix it later\n",
    " 'Brent South':'Brent Central',\n",
    " 'Dagenham':'Dagenham and Rainham',\n",
    " \"Ealing Acton and Shepherd'S Bush\":'Ealing Central and Acton',\n",
    " 'Hammersmith and Fulham':'Hammersmith',\n",
    " 'Hampstead and Highgate':'Hampstead and Kilburn',\n",
    " 'Hornchurch':'Hornchurch and Upminster',\n",
    " 'Kensington and Chelsea':'Kensington',\n",
    " 'Lewisham West':'Lewisham West and Penge',\n",
    " 'North Southwark and Bermondsey':'Bermondsey and Old Southwark',\n",
    " 'Poplar and Canning Town':'Poplar and Limehouse',\n",
    " \"Regent'S Park and Kensington North\":'Westminster North',\n",
    " 'Ruislip Northwood':'Ruislip Northwood and Pinner',\n",
    " 'Upminster':'Hornchurch and Upminster',\n",
    " 'Uxbridge':'Uxbridge and South Ruislip'}\n",
    "\n",
    "election_data.replace({\"constituency_name\" : dict_replace_old_constituencies},inplace=True)\n",
    "\n",
    "# Remap to account for different or out-of-date codes\n",
    "\n",
    "election_data.constituency_id = election_data.constituency_name.map(dict_name_code)\n",
    "\n",
    "msno.matrix(election_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c9baf8-8462-47f9-8954-4f4483c0b1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "election_data.hist(figsize=(15,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b41cb2-ba02-46d9-8e13-9a5b05b88cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lib_votes distribution also skews, so use median to replace\n",
    "\n",
    "election_data.fillna(election_data.median(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c8f5fa-000b-41be-97c1-cfd92e88a31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Determine winner by year\n",
    "\n",
    "def determine_winner(row):\n",
    "    vote_options = [row.con_votes, row.lib_votes, row.lab_votes]\n",
    "    win = np.argmax(vote_options)\n",
    "    if win == 0:\n",
    "        winner = \"con\"\n",
    "    elif win == 1:\n",
    "        winner = \"lib\"\n",
    "    elif win == 2:\n",
    "        winner = \"lab\"\n",
    "    return winner\n",
    "        \n",
    "election_data[\"winner\"] = election_data.apply(lambda row: determine_winner(row), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66403761-ec5a-461f-bb54-1a62c241c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "constituencies_boroughs = pd.read_csv(\"constituencies_boroughs.csv\")\n",
    "dict_constituencies_boroughs = dict(zip(constituencies_boroughs.Constituency,constituencies_boroughs.Borough))\n",
    "\n",
    "election_data[\"borough_name\"]=election_data[\"constituency_name\"].map(dict_constituencies_boroughs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea911545-8d31-4b9e-83f5-2c22b5c56d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.merge(election_data, house_data, how=\"inner\", on=[\"borough_name\", \"election\"])\n",
    "msno.matrix(combined_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b788a7-b820-49a4-bde5-76142fe9fae6",
   "metadata": {},
   "source": [
    "## Potential visualisation section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f9c7e5-5709-4b02-a280-a9fe4a141886",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(combined_data.winner, label=\"Count\",  palette=[\"#FF0000\",\"#00008B\", \"#FFFF00\"])    \n",
    "La,Co,Li = combined_data.winner.value_counts()\n",
    "print('Number of Labour: ',La)\n",
    "print('Number of Conservative : ',Co)\n",
    "print('Number of Liberal Democrat : ',Li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1761615-e7f5-4057-8c62-16d8f9eeb0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.hist(figsize=(15,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bee46d-577e-46b4-9374-9beb60b00120",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.plot(kind=\"density\", layout=(6,5), \n",
    "             subplots=True,sharex=False, sharey=False, figsize=(15,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff4dac3-99aa-45d1-81d7-cf93f486bb9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73843b9b-5150-4e61-8fbd-cb40b93729b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We should do some visualisation of \"final\" prepared dataset for feature selection? or do it earlier? let me think ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df3b388-d4a6-4190-a50c-c84cb0c3651c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You also need to do feature selection, outlier removal etc ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd376173-7a90-4a61-8d19-e914c88d44df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_ml = combined_data.drop(columns = ['constituency_name', 'constituency_id', 'con_votes', 'lib_votes', 'lab_votes', 'election',\n",
    "       'borough_name', 'code', 'year'] ,inplace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f752c33f-39e3-4ef7-801d-ca3d445609c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_ml.dtypes\n",
    "#Split post-2019 and pre-2019 everything pre-2019 election results is train basically\n",
    "pre = data_for_ml[combined_data.election != \"2019\"] #No \"election\" column in ML dataset, but can use information from original combined_dataset\n",
    "post = data_for_ml[combined_data.election == \"2019\"]\n",
    "# These are all numerical float variables\n",
    "# https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/\n",
    "# According to this, use ANOVA -> You need to split pre here to do feature selection or youll cross contaminate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470053a4-38ae-4995-8ee7-6d8bdc85f177",
   "metadata": {},
   "outputs": [],
   "source": [
    "Billy to do:\n",
    "    Accuracy obtained by predictions (especialy 2019 predicted vs actual) is consistently exactly reproducible. Why? (Have a proper think of what you're actually doing)\n",
    "    Write some code to auto-select best optuna resuts (normalised) and non normalised, and use accordingly (make sure to normalise the data if necessary!)\n",
    "    Visualisation/feature selection (even feature creation?) Remember goal is to get cross val accuracy as high as possible (predicted vs actual doesnt need to be as accurate as possible because we want to know how different brexit made things, but the validation results do need to be accurate!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e351ba3-cfae-4550-abf9-b82f39fcb62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pre.drop(columns=\"winner\") #wierd indexing problem here. Same as the reset index earlier, not solved though. Annyoing but ok\n",
    "y = pre.winner\n",
    "\n",
    "X_pre_feature_select, X_val, y_pre_feature_select, y_val = train_test_split(X, y, stratify=y, random_state=0)\n",
    "\n",
    "\n",
    "selector = SelectKBest(f_classif, k=4)\n",
    "select = selector.fit(X_pre_feature_select, y_pre_feature_select)\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "scores /= scores.max()\n",
    "print(scores)\n",
    "\n",
    "X_indices = np.arange(X_pre_feature_select.shape[-1])\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "plt.bar(X_indices - 0.05, scores, width=0.2)\n",
    "plt.title(\"Feature univariate score\")\n",
    "plt.xlabel(\"Feature number\")\n",
    "plt.ylabel(r\"Univariate score ($-Log(p_{value})$)\")\n",
    "plt.show()\n",
    "\n",
    "#REALLY, AREA SIZE IS BIGGEST FACTOR??? MORE THAN SALARY .... HMMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d65675c-cc3e-4058-a707-3de18b148c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['recycling_pct','population_size','area_size','no_of_houses']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbf281f-f96f-47eb-afd3-0a6e84901f66",
   "metadata": {},
   "source": [
    "## Here is the ML section, you are going to predict the 2019 results and compare them to the real 2019 and see the effect of Brexit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0a79de-005b-43a4-a44f-d192eab1c905",
   "metadata": {},
   "source": [
    "Billy, you need to get this accuracy in the validation section as high as possible (via feature selection, outlier removal etc), because that model will be used to predict 2019. So quality of analysis depnds on this\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9e9296-2954-408a-bf04-25271f583b03",
   "metadata": {},
   "source": [
    "### ML with normalised data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e935a6-6192-454f-b818-dd0e2b494eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_val[selected_features]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_val, y_val, test_size=0.2, stratify=y_val,random_state=r_s)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#YOU NEED TO NORMALIZE THE DATA HERE BECAUSE OF THE SVM! EVEN THOUGH YOU DONT NEED TO FOR RANDOM FOREST OR ACTUALLY MAYBE SPLIT OPTUNA INTO TREE AND NON-TREE, SO SPLIT BETWEEN DATA THAT NEEDS NORMALIZED AND DOESNT\n",
    "\n",
    "#Step 1. Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "\n",
    "    classifier_name = trial.suggest_categorical(\"classifier\", [\"SVC\", \"KNeighbors\"])\n",
    "    \n",
    "    # Step 2. Setup values for the hyperparameters:\n",
    "        \n",
    "    if classifier_name == \"SVC\":\n",
    "        svc_c = trial.suggest_float(\"svc_c\", 1e-10, 1e10, log=True)\n",
    "        classifier_object = SVC(C=svc_c, gamma=\"auto\")    \n",
    "\n",
    "    else:\n",
    "        n_neighbors = trial.suggest_int(\"n_neighbors\", 1, 30)\n",
    "        weights = trial.suggest_categorical(\"weights\", ['uniform', 'distance'])\n",
    "        classifier_object = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights)\n",
    "\n",
    "    # Step 3: Scoring method:\n",
    "    classifier_object.fit(X_train, y_train)\n",
    "    preds = classifier_object.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "    return accuracy\n",
    "\n",
    "# Step 4: Running it\n",
    "optuna.logging.set_verbosity(optuna.logging.CRITICAL)\n",
    "svc_knn_study = optuna.create_study(direction=\"maximize\")\n",
    "svc_knn_study.optimize(objective, n_trials=1000)\n",
    "print(svc_knn_study.best_trial)\n",
    "print(svc_knn_study.best_trial.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97f5efb-6095-4f98-bfcb-4a647aa643b0",
   "metadata": {},
   "source": [
    "### No normalisation ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308c232a-12a1-47c3-b1c2-44d43012afee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for more complex, lightgbm and xgboost\n",
    "\n",
    "#Start simple, then when you get the hang of it, follow from here\n",
    "\n",
    "#https://towardsdatascience.com/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5  (he does CV within the objective call, which is what I wanted to know! So you CAN do it)\n",
    "#https://towardsdatascience.com/how-to-beat-the-heck-out-of-xgboost-with-lightgbm-comprehensive-tutorial-5eba5219599\n",
    "\n",
    "#This is the one you ended up basing yourself on: https://practicaldatascience.co.uk/machine-learning/how-to-tune-a-lightgbmclassifier-model-with-optuna\n",
    "\n",
    "X_val = X_val[selected_features]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_val, y_val, test_size=0.2, stratify=y_val, random_state=r_s)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function to be minimized.\n",
    "    \"\"\"\n",
    "\n",
    "    classifier_name = trial.suggest_categorical(\"classifier\", [\"LightGBM\", \"RandomForest\"])\n",
    "\n",
    "    if classifier_name == \"LightGBM\":\n",
    "    \n",
    "        param = {\n",
    "            \"objective\": \"multiclass\",\n",
    "            \"metric\": \"multi_logloss\",\n",
    "            \"verbosity\": -1, \n",
    "            \"boosting_type\": \"gbdt\",\n",
    "            \"num_class\": 3,\n",
    "            \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "            \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "            \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
    "            \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
    "            \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),}\n",
    "            \n",
    "        classifier_object = lgbm.LGBMClassifier(**param)\n",
    "            \n",
    "    else:\n",
    "        rf_n_estimators = trial.suggest_int(\"rf_n_estimators\", 10, 1000)\n",
    "        rf_max_depth = trial.suggest_int(\"rf_max_depth\", 2, 32, log=True)\n",
    "        classifier_object = RandomForestClassifier(max_depth=rf_max_depth, n_estimators=rf_n_estimators)\n",
    "        \n",
    "        \n",
    "    classifier_object.fit(X_train, y_train)\n",
    "    preds = classifier_object.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "    return accuracy\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.CRITICAL)\n",
    "rf_lgbm_study = optuna.create_study(study_name=\"rf_lightgbm\", direction=\"maximize\")\n",
    "rf_lgbm_study.optimize(objective, n_trials=1000)\n",
    "print(rf_lgbm_study.best_trial)\n",
    "print(rf_lgbm_study.best_trial.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff69f9f5-c57d-4d87-906c-d20e74552c6a",
   "metadata": {},
   "source": [
    "## Use best results from validation to predict 2019 results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863773a0-317b-4c1a-8f99-a14676a83e4a",
   "metadata": {},
   "source": [
    "Best results are obtained using LightGBM, so let's use a LightGBM model. NOT NECESSARILY, WRITE SOME EASY IF CODE TO CHECK THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b5ebf7-fcc2-48c0-aeab-273d0e138e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre = data_for_ml[combined_data.election != \"2019\"] #No \"election\" column in ML dataset, but can use information from original combined_dataset\n",
    "# post = data_for_ml[combined_data.election == \"2019\"]\n",
    "\n",
    "X_train = pre.drop(columns=\"winner\") #Train on all the pre data (so feature selection data and validation data)\n",
    "y_train = pre.winner\n",
    "\n",
    "X_test = post.drop(columns=\"winner\") #Test in all post data\n",
    "\n",
    "param = rf_lgbm_study.best_params\n",
    "\n",
    "#classifier_object = lgbm.LGBMClassifier(**param)\n",
    "classifier_object = RandomForestClassifier(n_estimators= 31, max_depth= 5)\n",
    "\n",
    "\n",
    "classifier_object.fit(X_train, y_train)\n",
    "preds = classifier_object.predict(X_test)\n",
    "#How accurate were my predictions?\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba6249f-0fa9-4f64-bf37-12de8af4005e",
   "metadata": {},
   "source": [
    "Add predicted results to combined_data frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce864fe-15a6-441e-9251-615fd477c5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_2019_frame = pd.DataFrame(combined_data[combined_data.election == \"2019\"])\n",
    "\n",
    "predicted_2019_frame[\"winner\"] = preds\n",
    "predicted_2019_frame[\"election\"] = \"2019 Predicted\"\n",
    "\n",
    "\n",
    "\n",
    "# Bit of fudging here. Need to rethink classification at some point but for now this makes sense logically. (Only consider actual election year results)\n",
    "\n",
    "assign_list = []\n",
    "for group in predicted_2019_frame.groupby('constituency_name'):\n",
    "    if group[1].winner.iloc[0] != group[1].winner.iloc[1]:\n",
    "        group[1].winner.iloc[0] = group[1].winner.iloc[1]\n",
    "    assign_list.append(group[1].winner)\n",
    "\n",
    "predicted_2019_frame[\"winner\"] = pd.concat(assign_list)\n",
    "combined_data = pd.concat([combined_data,predicted_2019_frame])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178c13f4-09b0-4533-b2fa-02fa5f51bc71",
   "metadata": {},
   "source": [
    "Link the final dataset to the polygon information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1975c777-7279-464b-b4e9-4559e695da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapefile = gpd.read_file(\"Westminster_Parliamentary_Constituencies_(Dec_2021)_UK_BUC/Westminster_Parliamentary_Constituencies_(Dec_2021)_UK_BUC.shp\")\n",
    "\n",
    "shapefile.rename(columns={\"PCON21CD\": \"constituency_id\"},inplace=True)\n",
    "\n",
    "print(shapefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd3bc65-453e-43b8-9a73-d99001de5c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_combined_data = pd.merge(combined_data, shapefile[[\"constituency_id\", \"geometry\"]], how=\"inner\", on= \"constituency_id\")\n",
    "geo_combined_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2a5988-1f85-4ad3-9692-a0f263281756",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_combined_data.drop(columns=[\"code\",\"borough_flag\", \"borough_name\", \"year\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71d547b-d79b-4d2f-b533-de94e2a1e179",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_combined_data = gpd.GeoDataFrame(geo_combined_data) #Can you get away with only dropping coords here?\n",
    "geo_combined_data.set_crs(\"epsg:27700\",inplace=True) #This line is importtant, Billy explain what this is and how you obtained it! Basically its encoding infrormation and if you dont include it wont work\\n\",\n",
    "\n",
    "geo_combined_data.to_file('LondonFiles.shp', driver='ESRI Shapefile') #not sure we need the driver, check. THIS IS FINAL OUTPUT FOR TABLEAU\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "45f8b11c08f6a59f9890e073ce28e986369b4efc743c5634e894976e10ccbf22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
