{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae39d5d-e635-4ba8-9e19-e5b32a5a18a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno as msno\n",
    "import plotly as py\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import optuna\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "from geopy.geocoders import Nominatim\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, log_loss, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "import time\n",
    "\n",
    "r_s = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea1bd3e-5a3f-4e46-adb1-28aca30eacb5",
   "metadata": {},
   "source": [
    "## Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5896ec1-2ec2-44ee-ac42-f6340d24f85e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load in housing data\n",
    "\n",
    "house_data = pd.read_csv(\"housing_in_london/housing_in_london_yearly_variables.csv\") #billy you need to cut sub 2001 data?\n",
    "house_data.rename(columns={\"area\": \"borough_name\"}, inplace=True)\n",
    "house_data.borough_name = house_data.borough_name.str.title()\n",
    "house_data.borough_name = house_data.borough_name.replace(['Of','And'],['of','and'], regex=True)\n",
    "house_data.dtypes # Lets check datatypes. FOr some reason mean salary and recylcing percentages are objects not floats. Lets change that!\n",
    "house_data.mean_salary = house_data.mean_salary.apply(pd.to_numeric, errors='coerce') #Drop strings and make float\n",
    "house_data.recycling_pct = house_data.recycling_pct.apply(pd.to_numeric, errors='coerce') #Drop strings and make float\n",
    "msno.matrix(house_data)\n",
    "\n",
    "# Let's first drop the entire \"life_satisfaction\" column here as it is missing lots of values. We can then remove individual rows that contain a \"NaN.\n",
    "house_data.drop(columns = [\"life_satisfaction\"], inplace = True)\n",
    "\n",
    "# Change dates to just years\n",
    "house_data['year'] = house_data['date'].str.split('-').str[0]\n",
    "house_data.drop(columns = [\"date\"], inplace = True)\n",
    "\n",
    "house_data.dropna(inplace = True)\n",
    "house_data=house_data[(house_data.borough_name != 'London') & (house_data.borough_name != 'England' )]\n",
    "\n",
    "# Do this quickly, optimise later\n",
    "\n",
    "def add_election(row):\n",
    "    if int(row.year) < 2006:\n",
    "        election = \"2005\"\n",
    "        return election\n",
    "    elif int(row.year) < 2011:\n",
    "        election = \"2010\"\n",
    "        return election\n",
    "    elif int(row.year) < 2016:\n",
    "        election = \"2015\"\n",
    "        return election\n",
    "    elif int(row.year) < 2018:\n",
    "        election = \"2017\"\n",
    "        return election\n",
    "    else:\n",
    "        election = \"2019\"\n",
    "        return election\n",
    "        \n",
    "house_data[\"election\"] = house_data.apply(lambda row: add_election(row), axis=1)\n",
    "\n",
    "\n",
    "msno.matrix(house_data)\n",
    "#Done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4fe34a-cdb6-460d-bfc1-b443242890bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in election data\n",
    "\n",
    "election_data = pd.read_csv(\"1918-2019election_results.csv\",encoding = \"ISO-8859-1\") #Needs encoding specified or wont read file\n",
    "election_data = election_data[election_data[\"election\"]>\"2001\"] #this now matches other data\n",
    "election_data = election_data[election_data[\"country/region\"]=='London'] #Only London\n",
    "election_data = election_data[[\"constituency_id\", \"constituency_name\", \"con_share\", \"lib_share\", \"lab_share\", \"election\"]] # We want the share (percentage of votes rather than raw number)\n",
    "election_data.constituency_name = election_data.constituency_name.replace(['&','And'],'and', regex=True)\n",
    "election_data.constituency_name = election_data.constituency_name.replace('Of','of', regex=True)\n",
    "#Load in constituency list for conversion\n",
    "\n",
    "constituencies = pd.read_excel(\"PCON_DEC_2021_UK_NC.xlsx\", sheet_name=\"PCON_DEC_2021_UK_NC\", names=[\"constituency_code\", \"constituency_name\"])\n",
    "constituencies.constituency_name = constituencies.constituency_name.replace(',','', regex=True)\n",
    "dict_name_code = dict(zip(constituencies.constituency_name,constituencies.constituency_code))\n",
    "\n",
    "# Remap to account for different or out-of-date codes\n",
    "\n",
    "election_data.constituency_id = election_data.constituency_name.map(dict_name_code)\n",
    "\n",
    "msno.matrix(election_data)\n",
    "#Still some NaNs will clean this up in future work,for now just drop!\n",
    "election_data.dropna(inplace = True)\n",
    "\n",
    "#Determine winner by year\n",
    "\n",
    "def determine_winner(row):\n",
    "    vote_shares = [row.con_share, row.lib_share, row.lab_share]\n",
    "    win = np.argmax(vote_shares)\n",
    "    if win == 0:\n",
    "        winner = \"con\"\n",
    "    elif win == 1:\n",
    "        winner = \"lib\"\n",
    "    elif win == 2:\n",
    "        winner = \"lab\"\n",
    "    return winner\n",
    "        \n",
    "election_data[\"winner\"] = election_data.apply(lambda row: determine_winner(row), axis=1)\n",
    "\n",
    "msno.matrix(election_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66403761-ec5a-461f-bb54-1a62c241c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "constituencies_boroughs = pd.read_csv(\"constituencies_boroughs.csv\")\n",
    "dict_constituencies_boroughs = dict(zip(constituencies_boroughs.Constituency,constituencies_boroughs.Borough))\n",
    "\n",
    "election_data[\"borough_name\"]=election_data[\"constituency_name\"].map(dict_constituencies_boroughs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e52965-cd65-4078-8ae2-aed321cbcd11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load in geo/polygon data YOU MAY NOT NEED THIS YET\n",
    "\n",
    "geo_data = gpd.read_file('BoroughFiles/London_Borough_Excluding_MHW.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b7e816-0ae1-416d-963f-f53389e44d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.merge(election_data, house_data, how=\"outer\", on=[\"borough_name\", \"election\"])\n",
    "\n",
    "#Drop Nan (There shouldnt be any, but just in case!)\n",
    "combined_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc28965-82e9-48f3-a4f4-6719fa08b1bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #No, we do need some kind of dummy data. Think of a smarter way to do this in future\n",
    "\n",
    "# # Some manual adds of dummy data for the sake of Tableau plotting\n",
    "\n",
    "# #2019 dummy data\n",
    "# bexley_2019_dummy = combined_data.iloc[938,:]\n",
    "# bexley_2019_dummy[[\"election\",\"winner\",\"year\"]] = [\"2019\",\"con\", \"2018\"]\n",
    "# redbridge_2019_dummy = combined_data.iloc[956,:]\n",
    "# redbridge_2019_dummy[[\"election\",\"winner\",\"year\"]] = [\"2019\",\"lab\", \"2018\"]\n",
    "# kingston_upon_thames_2019_dummy = combined_data.iloc[1017,:]\n",
    "# kingston_upon_thames_2019_dummy[[\"election\",\"winner\",\"year\"]] = [\"2019\",\"lib\", \"2018\"]\n",
    "# merton_2019_dummy = combined_data.iloc[1027,:]\n",
    "# merton_2019_dummy[[\"election\",\"winner\",\"year\"]] = [\"2019\",\"con\", \"2018\"]\n",
    "\n",
    "# westminster_2005_dummy = combined_data.iloc[590,:]\n",
    "# westminster_2005_dummy[[\"election\",\"winner\",\"year\"]] = [\"2005\",\"con\", \"2004\"]\n",
    "# kensington_and_chelsea_2005_dummy = combined_data.iloc[540,:]\n",
    "# kensington_and_chelsea_2005_dummy[[\"election\",\"winner\",\"year\"]] = [\"2005\",\"con\", \"2004\"]\n",
    "# hammersmith_and_fulham_2005_dummy = combined_data.iloc[496,:]\n",
    "# hammersmith_and_fulham_2005_dummy[[\"election\",\"winner\",\"year\"]] = [\"2005\",\"con\", \"2004\"]\n",
    "\n",
    "# dummy_df = pd.DataFrame([bexley_2019_dummy, redbridge_2019_dummy, kingston_upon_thames_2019_dummy, merton_2019_dummy, westminster_2005_dummy, kensington_and_chelsea_2005_dummy, hammersmith_and_fulham_2005_dummy])\n",
    "# combined_data = pd.concat([combined_data,dummy_df])\n",
    "\n",
    "# #combined_data.reset_index(inplace=True, drop=True) # Do I need this? Yeah maybe, you added a bug somewhere ... noworries, fix tomorrow!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b788a7-b820-49a4-bde5-76142fe9fae6",
   "metadata": {},
   "source": [
    "## Potential visualisation section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73843b9b-5150-4e61-8fbd-cb40b93729b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We should do some visualisation of \"final\" prepared dataset for feature selection? or do it earlier? let me think ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbf281f-f96f-47eb-afd3-0a6e84901f66",
   "metadata": {},
   "source": [
    "## Here is the ML section, you are going to predict the 2019 results and compare them to the real 2019 and see the effect of Brexit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aed753e-7389-465e-a22a-e03624303f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_ml = combined_data.drop(columns = ['constituency_name', 'constituency_id', 'con_share', 'lib_share', 'lab_share', 'election',\n",
    "       'borough_name', 'code', 'borough_flag', 'year'] ,inplace = False)\n",
    "\n",
    "#Split post-2019 and pre-2019 everything pre-2019 election results is train basically\n",
    "pre = data_for_ml[combined_data.election != \"2019\"] #No \"election\" column in ML dataset, but can use information from original combined_dataset\n",
    "post = data_for_ml[combined_data.election == \"2019\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9e9296-2954-408a-bf04-25271f583b03",
   "metadata": {},
   "source": [
    "### Simple ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e935a6-6192-454f-b818-dd0e2b494eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pre.drop(columns=\"winner\") #wierd indexing problem here. Same as the reset index earlier, not solved though. Annyoing but ok\n",
    "y = pre.winner\n",
    "\n",
    "#Step 1. Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "\n",
    "    classifier_name = trial.suggest_categorical(\"classifier\", [\"SVC\", \"RandomForest\"])\n",
    "    \n",
    "    # Step 2. Setup values for the hyperparameters:\n",
    "        \n",
    "    if classifier_name == \"SVC\":\n",
    "        svc_c = trial.suggest_float(\"svc_c\", 1e-10, 1e10, log=True)\n",
    "        classifier_obj = SVC(C=svc_c, gamma=\"auto\")    \n",
    "\n",
    "    else:\n",
    "        rf_n_estimators = trial.suggest_int(\"rf_n_estimators\", 10, 1000)\n",
    "        rf_max_depth = trial.suggest_int(\"rf_max_depth\", 2, 32, log=True)\n",
    "        classifier_obj = RandomForestClassifier(\n",
    "            max_depth=rf_max_depth, n_estimators=rf_n_estimators)\n",
    "\n",
    "    # Step 3: Scoring method:\n",
    "    score = cross_val_score(classifier_obj, X, y, n_jobs=-1, cv=5)\n",
    "    accuracy = score.mean()\n",
    "    return accuracy\n",
    "\n",
    "# Step 4: Running it\n",
    "optuna.logging.set_verbosity(optuna.logging.CRITICAL)\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "print(study.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97f5efb-6095-4f98-bfcb-4a647aa643b0",
   "metadata": {},
   "source": [
    "### More complex ML (add XGBoost later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308c232a-12a1-47c3-b1c2-44d43012afee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for more complex, lightgbm and xgboost\n",
    "\n",
    "#Start simple, then when you get the hang of it, follow from here\n",
    "\n",
    "#https://towardsdatascience.com/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5  (he does CV within the objective call, which is what I wanted to know! So you CAN do it)\n",
    "#https://towardsdatascience.com/how-to-beat-the-heck-out-of-xgboost-with-lightgbm-comprehensive-tutorial-5eba5219599\n",
    "\n",
    "#This is the one you ended up basing yourself on: https://practicaldatascience.co.uk/machine-learning/how-to-tune-a-lightgbmclassifier-model-with-optuna\n",
    "\n",
    "X = pre.drop(columns=\"winner\") #wierd indexing problem here. Same as the reset index earlier, not solved though. Annyoing but ok\n",
    "y = pre.winner\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function to be minimized.\n",
    "    \"\"\"\n",
    "    param = {\n",
    "        \"objective\": \"multiclass\",\n",
    "        \"metric\": \"multi_logloss\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"num_class\": 3,\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "    }\n",
    "    gbm = lgb.LGBMClassifier(**param)\n",
    "    gbm.fit(X_train, y_train, verbose=False)\n",
    "    preds = gbm.predict(X_test, verbose=-100)\n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "    return accuracy\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.CRITICAL)\n",
    "study = optuna.create_study(study_name=\"lightgbm\", direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "print(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cb99ec-d0e5-4ffd-806e-0613ffb8b8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Billy in this block add XGBoost in the future for comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
