{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae39d5d-e635-4ba8-9e19-e5b32a5a18a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno as msno\n",
    "import plotly as py\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from geopy.geocoders import Nominatim\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5896ec1-2ec2-44ee-ac42-f6340d24f85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in housing data\n",
    "\n",
    "house_data = pd.read_csv(\"housing_in_london/housing_in_london_yearly_variables.csv\") #billy you need to cut sub 2001 data?\n",
    "house_data.dtypes # Lets check datatypes. FOr some reason mean salary and recylcing percentages are objects not floats. Lets change that!\n",
    "house_data.mean_salary = house_data.mean_salary.apply(pd.to_numeric, errors='coerce') #Drop strings and make float\n",
    "house_data.recycling_pct = house_data.recycling_pct.apply(pd.to_numeric, errors='coerce') #Drop strings and make float\n",
    "house_data.area.unique() #Drop non boroughs\n",
    "house_data.date.unique() #Match election dates\n",
    "msno.matrix(house_data)\n",
    "\n",
    "# Let's first drop the entire \"life_satisfaction\" column here as it is missing lots of values. We can then remove individual rows that contain a \"NaN.\n",
    "house_data.drop(columns = [\"life_satisfaction\"], inplace = True)\n",
    "\n",
    "# Change dates to just years\n",
    "house_data['year'] = house_data['date'].str.split('-').str[0]\n",
    "house_data.drop(columns = [\"date\"], inplace = True)\n",
    "\n",
    "house_data.dropna(inplace = True)\n",
    "house_data=house_data[(house_data.area != 'london') & (house_data.area != 'england' )]\n",
    "\n",
    "msno.matrix(house_data)\n",
    "#Done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4fe34a-cdb6-460d-bfc1-b443242890bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in election data\n",
    "\n",
    "election_data = pd.read_csv(\"1918-2019election_results.csv\",encoding = \"ISO-8859-1\") #Needs encoding specified or wont read file\n",
    "election_data = election_data[election_data.election>\"1999\"] #this now matches other data\n",
    "election_data = election_data[election_data[\"country/region\"]=='London'] #Only London\n",
    "election_data = election_data[[\"constituency_name\", \"con_share\", \"lib_share\", \"lab_share\", \"election\"]] # We want the share (percentage of votes rather than raw number)\n",
    "\n",
    "#Load in constituency list for conversion\n",
    "\n",
    "constituencies = pd.read_csv(\"constituencies_boroughs.csv\")\n",
    "dict_constituencies_boroughs = dict(constituencies.values)\n",
    "\n",
    "#Conversion from constituency to area\n",
    "\n",
    "election_data['area']= election_data[\"constituency_name\"].map(dict_constituencies_boroughs)\n",
    "election_data['area']= election_data.area.str.lower() #lower case for now so it works on join\n",
    "msno.matrix(election_data)\n",
    "#Still some NaNs will clean this up in future work,for now just drop!\n",
    "election_data.dropna(inplace = True)\n",
    "\n",
    "#Determine winner by year\n",
    "\n",
    "def determine_winner(row):\n",
    "    vote_shares = [row.con_share, row.lib_share, row.lab_share]\n",
    "    win = np.argmax(vote_shares)\n",
    "    if win == 0:\n",
    "        winner = \"con\"\n",
    "    elif win == 1:\n",
    "        winner = \"lib\"\n",
    "    elif win == 2:\n",
    "        winner = \"lab\"\n",
    "    return winner\n",
    "        \n",
    "election_data[\"winner\"] = election_data.apply(lambda row: determine_winner(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e52965-cd65-4078-8ae2-aed321cbcd11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load in geo/polygon data\n",
    "\n",
    "geo_data = gpd.read_file('BoroughFiles/London_Borough_Excluding_MHW.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fe9a0e-cba8-4a54-9975-0510af34dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do this quickly, optimise later\n",
    "\n",
    "def add_election(row):\n",
    "    if int(row.year) < 2006:\n",
    "        election = \"2005\"\n",
    "        return election\n",
    "    elif int(row.year) < 2011:\n",
    "        election = \"2010\"\n",
    "        return election\n",
    "    elif int(row.year) < 2016:\n",
    "        election = \"2015\"\n",
    "        return election\n",
    "    elif int(row.year) < 2018:\n",
    "        election = \"2017\"\n",
    "        return election\n",
    "    else:\n",
    "        election = \"2019\"\n",
    "        return election\n",
    "        \n",
    "house_data[\"election\"] = house_data.apply(lambda row: add_election(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b7e816-0ae1-416d-963f-f53389e44d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do an inner(?) merge here\n",
    "\n",
    "combined_data = pd.merge(election_data, house_data, how=\"inner\", on=[\"area\", \"election\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc28965-82e9-48f3-a4f4-6719fa08b1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some manual adds of dummy data for the sake of Tableau plotting\n",
    "\n",
    "#2019 dummy data\n",
    "bexley_2019_dummy = combined_data.iloc[938,:]\n",
    "bexley_2019_dummy[[\"election\",\"winner\",\"year\"]] = [\"2019\",\"con\", \"2018\"]\n",
    "redbridge_2019_dummy = combined_data.iloc[956,:]\n",
    "redbridge_2019_dummy[[\"election\",\"winner\",\"year\"]] = [\"2019\",\"lab\", \"2018\"]\n",
    "kingston_upon_thames_2019_dummy = combined_data.iloc[1017,:]\n",
    "kingston_upon_thames_2019_dummy[[\"election\",\"winner\",\"year\"]] = [\"2019\",\"lib\", \"2018\"]\n",
    "merton_2019_dummy = combined_data.iloc[1027,:]\n",
    "merton_2019_dummy[[\"election\",\"winner\",\"year\"]] = [\"2019\",\"con\", \"2018\"]\n",
    "\n",
    "#2005 dummy data\n",
    "westminster_2005_dummy = combined_data.iloc[590,:]\n",
    "westminster_2005_dummy[[\"election\",\"winner\",\"year\"]] = [\"2005\",\"con\", \"2004\"]\n",
    "kensington_and_chelsea_2005_dummy = combined_data.iloc[540,:]\n",
    "kensington_and_chelsea_2005_dummy[[\"election\",\"winner\",\"year\"]] = [\"2005\",\"con\", \"2004\"]\n",
    "hammersmith_and_fulham_2005_dummy = combined_data.iloc[496,:]\n",
    "hammersmith_and_fulham_2005_dummy[[\"election\",\"winner\",\"year\"]] = [\"2005\",\"con\", \"2004\"]\n",
    "\n",
    "dummy_df = pd.DataFrame([bexley_2019_dummy, redbridge_2019_dummy, kingston_upon_thames_2019_dummy, merton_2019_dummy, westminster_2005_dummy, kensington_and_chelsea_2005_dummy, hammersmith_and_fulham_2005_dummy])\n",
    "combined_data = pd.concat([combined_data,dummy_df])\n",
    "\n",
    "#combined_data.reset_index(inplace=True, drop=True) # Do I need this? Yeah maybe, you added a bug somewhere ... noworries, fix tomorrow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cb99ec-d0e5-4ffd-806e-0613ffb8b8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BILLY NEED TO CHECK THIS, WHY IS CROSS VAL ACC LOWER?\n",
    "\n",
    "# Here you are going to predict some elections. 2017 and 2019\n",
    "#Lets start with 2019\n",
    "\n",
    "data_for_ml = combined_data.drop(columns = ['constituency_name', 'con_share', 'lib_share', 'lab_share', 'election',\n",
    "       'area', 'code', 'borough_flag', 'year'] ,inplace = False)\n",
    "\n",
    "#Split post-2019 and pre-2019 everything pre-2019 election results is train basically\n",
    "pre = data_for_ml[combined_data.election != \"2019\"] #No \"election\" column in ML dataset, but can use information from original combined_dataset\n",
    "post = data_for_ml[combined_data.election == \"2019\"]\n",
    "\n",
    "# Lets begin with simple RF, then we can test out alternative like SVM (dont forget fit transform with SVM though!)\n",
    "\n",
    "# model = RandomForestClassifier(random_state=1,n_jobs=-1)\n",
    "\n",
    "#Lets do k-fold on train to check model is consistent\n",
    "#Add undersampling later! (And then remove stratified k-fold cos wont need it after undersampling)\n",
    "\n",
    "x_pre = np.asarray(pre.drop(columns=[\"winner\"]).astype(float)) #wierd indexing problem here. Same as the reset index earlier, not solved though. Annyoing but ok\n",
    "y_pre = np.asarray(pre.winner)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "conf_mat = np.zeros([3,3])\n",
    "accuracy_list = []\n",
    "for train_index, test_index in skf.split(x_pre, y_pre):\n",
    "    x_train_fold, x_test_fold = x_pre[train_index], x_pre[test_index]\n",
    "    y_train_fold, y_test_fold = y_pre[train_index], y_pre[test_index]\n",
    "    model = RandomForestClassifier(random_state=1,n_jobs=-1) #not necessary? but for peace of mind do this\n",
    "    model.fit(x_train_fold, y_train_fold)\n",
    "    #accuracy score\n",
    "    accuracy_list.append(model.score(x_test_fold, y_test_fold))\n",
    "    # prediction with test data\n",
    "    y_pred_fold = model.predict(x_test_fold)\n",
    "    # generate confusion matrix\n",
    "    conf_mat_fold = confusion_matrix(y_test_fold, y_pred_fold)\n",
    "    conf_mat += conf_mat_fold\n",
    "\n",
    "#Lets quickly get some predicitons for the dataset/tableau NOT UNDERSAMPLED OR ANYTHING YET\n",
    "\n",
    "x_post = np.asarray(post.drop(columns=[\"winner\"]).astype(float)) #wierd indexing problem here. Same as the reset index earlier, not solved though. Annyoing but ok\n",
    "y_post = np.asarray(post.winner)\n",
    "conf_mat = np.zeros([3,3])\n",
    "model = RandomForestClassifier(random_state=1,n_jobs=-1) #not necessary? but for peace of mind do this\n",
    "model.fit(x_pre, y_pre)\n",
    "#accuracy score\n",
    "acc = model.score(x_post, y_post)\n",
    "# prediction with test data\n",
    "y_pred = model.predict(x_post)\n",
    "# generate confusion matrix\n",
    "conf_mat = confusion_matrix(y_post, y_pred)\n",
    "\n",
    "\n",
    "#Lets add the pres to dataset/tableau\n",
    "predicted_data = pd.DataFrame(combined_data[combined_data.election == \"2019\"])\n",
    "predicted_data.election[predicted_data.election == \"2019\"] = \"2019_pred\"\n",
    "predicted_data.winner = y_pred\n",
    "combined_data = pd.concat([combined_data,predicted_data])\n",
    "\n",
    "print(accuracy_list)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a0cab9-a553-4ad5-856f-d00542d060f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data['area'].unique #How many areas?\n",
    "\n",
    "\n",
    "# It takes a long time to make the call, so lets just get the codes first, then manually add the missing ones, then do the geocoding\n",
    "geolocator = Nominatim(user_agent=\"london\")\n",
    "dict_area_coord = {}\n",
    "\n",
    "for area in combined_data['area'].unique():\n",
    "    dict_area_coord[area] = geolocator.geocode(query=area, country_codes=\"gb\", addressdetails = True) #Very important to specify country, otherwise missclassify, and also not find some\n",
    "\n",
    "#This loads the wrong Tower Hamelts, so for now let`s correct that manually\n",
    "dict_area_coord[\"tower hamlets\"] = geolocator.geocode(\"London Borough of Tower Hamlets\")\n",
    "    \n",
    "combined_data['coords'] = combined_data['area'].map(dict_area_coord) #If it turns out we can drop this safely, can you rewrite code to just remove coords from discussion?\n",
    "combined_data['latitude'] = combined_data['coords'].apply(lambda x: x[1][0])\n",
    "combined_data['longtitude'] = combined_data['coords'].apply(lambda x: x[1][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4b06ce-daa0-4951-9f1b-d815e7ac1f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last step\n",
    "\n",
    "area_polygon_dict = dict(zip(geo_data['NAME'].str.lower(), geo_data['geometry']))\n",
    "\n",
    "combined_data['geometry'] = combined_data['area'].map(area_polygon_dict)\n",
    "\n",
    "gdata = gpd.GeoDataFrame(combined_data.drop([\"coords\",\"latitude\",\"longtitude\"],axis = 1)) #Can you get away with only dropping coords here?\n",
    "gdata.set_crs(\"epsg:27700\",inplace=True) #This line is importtant, Billy explain what this is and how you obtained it! Basically its encoding infrormation and if you dont include it wont work\n",
    "\n",
    "gdata.to_file('LondonFiles.shp', driver='ESRI Shapefile') #not sure we need the driver, check. THIS IS FINAL OUTPUT FOR TABLEAU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
